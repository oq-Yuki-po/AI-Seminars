# E 資格まとめ {ignore=true}

## 目次 {ignore=true}

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [目標](#目標)
- [本題](#本題)
  - [深層学習](#深層学習)
    - [勾配消失問題](#勾配消失問題)
      - [勾配消失問題とは](#勾配消失問題とは)
      - [勾配消失問題の解決策](#勾配消失問題の解決策)
      - [1. 活性化関数の選択](#1-活性化関数の選択)
      - [2. 重みの初期値設定](#2-重みの初期値設定)
      - [3. バッチ正規化](#3-バッチ正規化)
    - [学習率最適化手法](#学習率最適化手法)
      - [モメンタム](#モメンタム)
      - [AdaGrad](#adagrad)
      - [RMSProp](#rmsprop)
      - [Adam](#adam)
    - [過学習](#過学習)
      - [過学習の原因](#過学習の原因)
      - [確認テスト](#確認テスト)
      - [過学習の対策](#過学習の対策)
    - [畳み込みニューラルネットワークの概念](#畳み込みニューラルネットワークの概念)
    - [最新の CNN](#最新の-cnn)
    - [再帰型ニューラルネットワークの概念](#再帰型ニューラルネットワークの概念)
    - [LSTM](#lstm)
    - [GRU](#gru)
    - [双方向 RNN](#双方向-rnn)
    - [Seq2Seq](#seq2seq)
    - [Word2vec](#word2vec)
    - [Attention Mechanism](#attention-mechanism)
    - [Tensorflow の実装演習](#tensorflow-の実装演習)
    - [強化学習](#強化学習)

<!-- /code_chunk_output -->

## 目標

E 資格の学習項目のまとめ

## 本題

### 深層学習

#### 勾配消失問題

##### 勾配消失問題とは

誤差逆伝播法が下位層(出力層から入力層)に向かって進んでいくにつれて勾配がどんどん緩やかになっていく  
そのため、勾配降下法による更新では下位層のパラメータはほとんど変わらず訓練は最適値に収束しなくなる

##### 勾配消失問題の解決策

1. 活性化関数の選択
2. 重みの初期値設定
3. バッチ正規化

##### 1. 活性化関数の選択

ReLU 関数を使用する  
勾配消失問題とスパース化に貢献することで良い成果をもたらしている

##### 2. 重みの初期値設定

- Xavier：重みの要素を前の層のノード数 n に対して、平均 0、標準偏差が 1/√n である正規分布から重みを設定する手法

- He：ReLU 関数に付随させ、よりもっと勾配消失を起こさないようにする。重みの要素を、ノード数 n に対して、平均 0、標準偏差が √2/n である正規分布から重みを設定

- 重みの初期値に 0 を設定すると、どのような問題が発生するか？
  すべての値(次に伝播する値)が同じ値で伝わるため、パラメータチューニング行われなくなる

##### 3. バッチ正規化

ミニバッチ単位へ入力値のデータの偏りを抑制する手法
活性化関数に値を渡す前後にバッチ正規化の処理を孕んだ層を加える
バッチ正規化の効果

1. 計算の高速化
2. 勾配消失が起こりづらくなる

#### 学習率最適化手法

学習率の決め方

- 大きすぎる：最適値にいつまでもたどり着かず、発散する
- 小さすぎる：収束するまで時間がかか利、大域局所最適値に収束しづらくなる

学習率最適化手法を利用して学習率を最適化する  
以下の４つについてまとめる

- モメンタム
- AdaGrad
- RMSProp
- Adam

##### モメンタム

誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性（0.05-0.09）の積を加算する
局所的最適解にならず、大域的最適解となる
谷間についてから最も低い位置(最適値)にいくまでの時間が早い

##### AdaGrad

学習が進むにつれて、学習係数を減衰させる手法。最初は大きく学習し、次第に小さく学習する。

##### RMSProp

Adagrad では学習が進むにつれて学習係数が小さくなってしまう問題があり、それを回避する手法。  
Adagrad の問題だった学習が進むにつれて分母の合計値は増えていき学習係数が 0 に近づいていくことを過去何回かの合計値を近似する指数移動平均を使用することで解消する。

##### Adam

移動平均で振動を抑制するモーメンタム と 学習率を調整して振動を抑制する RMSProp を組み合わせている

#### 過学習

テスト誤差と訓練誤差とで、学習曲線が乖離すること。  
特定の訓練サンプルに対して、特化して学習してしまうこと。  
原因：パラメータの数が多い、パラメータの値が適切でない、ノードが多いなど  
⇒ ネットワークの自由度が高いともいえる

##### 過学習の原因

- パラメータを大量に持ち、表現力の高いモデルである事
- 訓練データが少ない事

##### 確認テスト
問：機械学習の線形モデルの正則化はモデルの重みを制限することで可能になる。線形モデルの正則化手法の中にリッジ回帰という手法がある、その特徴は？

回答：ハイパーパラメータを大きな値に設定すると、すべての重みが限りなく0に近づく

##### 過学習の対策

- 正則化
ネットワークの自由度(層数、ノード数、パラメータ値etc...)を制約すること

- Weight decay
重みが大きい値をとることで、過学習が発生することがある  
誤差に対して、正則化項を加算することで、重みを抑制  
過学習がおこりそうな重みの大きさ以下で、  
重みをコントロールし、かつ重みの大きさにばらつきを出す必要がある

- ドロップアウト
ランダムにノードを削除して学習させること  
データ量を変化させずに、異なるモデルを学習させていると解釈できる


#### 畳み込みニューラルネットワークの概念

#### 最新の CNN

#### 再帰型ニューラルネットワークの概念

#### LSTM

#### GRU

#### 双方向 RNN

#### Seq2Seq

#### Word2vec

#### Attention Mechanism

#### Tensorflow の実装演習

#### 強化学習
