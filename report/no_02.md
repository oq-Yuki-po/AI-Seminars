# E資格まとめ {ignore=true} 

## 目次 {ignore=true} 


<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [目標](#目標)
- [本題](#本題)
  - [深層学習](#深層学習)
    - [勾配消失問題](#勾配消失問題)
      - [勾配消失問題とは](#勾配消失問題とは)
      - [勾配消失問題の解決策](#勾配消失問題の解決策)
      - [1. 活性化関数の選択](#1-活性化関数の選択)
      - [2. 重みの初期値設定](#2-重みの初期値設定)
      - [3. バッチ正規化](#3-バッチ正規化)
    - [学習率最適化手法](#学習率最適化手法)
      - [モメンタム](#モメンタム)
      - [AdaGrad](#adagrad)
      - [RMSProp](#rmsprop)
      - [Adam](#adam)
    - [過学習](#過学習)
    - [畳み込みニューラルネットワークの概念](#畳み込みニューラルネットワークの概念)
    - [最新のCNN](#最新のcnn)
    - [再帰型ニューラルネットワークの概念](#再帰型ニューラルネットワークの概念)
    - [LSTM](#lstm)
    - [GRU](#gru)
    - [双方向RNN](#双方向rnn)
    - [Seq2Seq](#seq2seq)
    - [Word2vec](#word2vec)
    - [Attention Mechanism](#attention-mechanism)
    - [Tensorflowの実装演習](#tensorflowの実装演習)
    - [強化学習](#強化学習)

<!-- /code_chunk_output -->


## 目標
E資格の学習項目のまとめ

## 本題

### 深層学習

#### 勾配消失問題

##### 勾配消失問題とは

誤差逆伝播法が下位層(出力層から入力層)に向かって進んでいくにつれて勾配がどんどん緩やかになっていく    
そのため、勾配降下法による更新では下位層のパラメータはほとんど変わらず訓練は最適値に収束しなくなる

##### 勾配消失問題の解決策
1. 活性化関数の選択
2. 重みの初期値設定
3. バッチ正規化

##### 1. 活性化関数の選択
ReLU関数を使用する  
勾配消失問題とスパース化に貢献することで良い成果をもたらしている

##### 2. 重みの初期値設定

- Xavier：重みの要素を前の層のノード数nに対して、平均0、標準偏差が1/√nである正規分布から重みを設定する手法

- He：ReLU関数に付随させ、よりもっと勾配消失を起こさないようにする。重みの要素を、ノード数nに対して、平均0、標準偏差が√2/nである正規分布から重みを設定

- 重みの初期値に0を設定すると、どのような問題が発生するか？
すべての値(次に伝播する値)が同じ値で伝わるため、パラメータチューニング行われなくなる

##### 3. バッチ正規化
ミニバッチ単位へ入力値のデータの偏りを抑制する手法
活性化関数に値を渡す前後にバッチ正規化の処理を孕んだ層を加える
バッチ正規化の効果
1. 計算の高速化
2. 勾配消失が起こりづらくなる

#### 学習率最適化手法

学習率の決め方

- 大きすぎる：最適値にいつまでもたどり着かず、発散する
- 小さすぎる：収束するまで時間がかか利、大域局所最適値に収束しづらくなる

学習率最適化手法を利用して学習率を最適化する  
以下の４つについてまとめる  
- モメンタム
- AdaGrad
- RMSProp
- Adam

##### モメンタム

##### AdaGrad

##### RMSProp

##### Adam


#### 過学習

テスト誤差と訓練誤差とで、学習曲線が乖離すること。  
特定の訓練サンプルに対して、特化して学習してしまうこと。  
原因：パラメータの数が多い、パラメータの値が適切でない、ノードが多いなど  
⇒ネットワークの自由度が高いともいえる

#### 畳み込みニューラルネットワークの概念

#### 最新のCNN

#### 再帰型ニューラルネットワークの概念

#### LSTM

#### GRU

#### 双方向RNN

#### Seq2Seq

#### Word2vec

#### Attention Mechanism

#### Tensorflowの実装演習

#### 強化学習
