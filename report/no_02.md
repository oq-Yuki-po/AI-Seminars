# E 資格まとめ {ignore=true}

## 目次 {ignore=true}

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [目標](#目標)
- [本題](#本題)
  - [深層学習](#深層学習)
    - [勾配消失問題](#勾配消失問題)
      - [確認テスト](#確認テスト)
      - [勾配消失問題とは](#勾配消失問題とは)
      - [勾配消失問題の解決策](#勾配消失問題の解決策)
      - [確認テスト](#確認テスト-1)
      - [1. 活性化関数の選択](#1-活性化関数の選択)
      - [2. 重みの初期値設定](#2-重みの初期値設定)
      - [確認テスト](#確認テスト-2)
      - [3. バッチ正規化](#3-バッチ正規化)
      - [演習](#演習)
    - [学習率最適化手法](#学習率最適化手法)
      - [モメンタム](#モメンタム)
      - [AdaGrad](#adagrad)
      - [RMSProp](#rmsprop)
      - [Adam](#adam)
      - [演習](#演習-1)
    - [過学習](#過学習)
      - [過学習の原因](#過学習の原因)
      - [確認テスト](#確認テスト-3)
      - [過学習の対策](#過学習の対策)
      - [確認テスト](#確認テスト-4)
      - [演習](#演習-2)
    - [畳み込みニューラルネットワークの概念](#畳み込みニューラルネットワークの概念)
      - [演習](#演習-3)
    - [最新の CNN](#最新の-cnn)
    - [再帰型ニューラルネットワークの概念](#再帰型ニューラルネットワークの概念)
    - [LSTM](#lstm)
    - [GRU](#gru)
    - [双方向 RNN](#双方向-rnn)
    - [Seq2Seq](#seq2seq)
    - [Word2vec](#word2vec)
    - [Attention Mechanism](#attention-mechanism)
    - [Tensorflow の実装演習](#tensorflow-の実装演習)
    - [強化学習](#強化学習)

<!-- /code_chunk_output -->

## 目標

E 資格の学習項目のまとめ

## 本題

### 深層学習

#### 勾配消失問題

##### 確認テスト
問：連鎖律の原理を使い、dz/dxを求めよ

![連鎖律](../images/_chain_rule.png "連鎖律")

##### 勾配消失問題とは

誤差逆伝播法が下位層(出力層から入力層)に向かって進んでいくにつれて勾配がどんどん緩やかになっていく  
そのため、勾配降下法による更新では下位層のパラメータはほとんど変わらず訓練は最適値に収束しなくなる

##### 勾配消失問題の解決策

1. 活性化関数の選択
2. 重みの初期値設定
3. バッチ正規化

##### 確認テスト
問：シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値は？

回答：

sigmoidの微分は下記  
(sigmoid)′=(1−sigmoid)(sigmoid)  
sigmoid関数は0.5で最大値を取るので  
(sigmoid)′=(1−0.5)(0.5)=0.25となる  


##### 1. 活性化関数の選択

ReLU 関数を使用する  
勾配消失問題とスパース化に貢献することで良い成果をもたらしている

##### 2. 重みの初期値設定

- Xavier：重みの要素を前の層のノード数 n に対して、平均 0、標準偏差が 1/√n である正規分布から重みを設定する手法

- He：ReLU 関数に付随させ、よりもっと勾配消失を起こさないようにする。重みの要素を、ノード数 n に対して、平均 0、標準偏差が √2/n である正規分布から重みを設定

##### 確認テスト
問： 重みの初期値に 0 を設定すると、どのような問題が発生するか？  
回答：すべての値(次に伝播する値)が同じ値で伝わるため、パラメータチューニング行われなくなる  

##### 3. バッチ正規化

ミニバッチ単位へ入力値のデータの偏りを抑制する手法
活性化関数に値を渡す前後にバッチ正規化の処理を孕んだ層を加える
バッチ正規化の効果

1. 計算の高速化
2. 勾配消失が起こりづらくなる

##### 演習

[演習結果](../pdf/vanishing_gradient.pdf "演習結果")

#### 学習率最適化手法

学習率の決め方

- 大きすぎる：最適値にいつまでもたどり着かず、発散する
- 小さすぎる：収束するまで時間がかか利、大域局所最適値に収束しづらくなる

学習率最適化手法を利用して学習率を最適化する  
以下の４つについてまとめる

- モメンタム
- AdaGrad
- RMSProp
- Adam

##### モメンタム

誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性（0.05-0.09）の積を加算する
局所的最適解にならず、大域的最適解となる
谷間についてから最も低い位置(最適値)にいくまでの時間が早い

##### AdaGrad

学習が進むにつれて、学習係数を減衰させる手法。最初は大きく学習し、次第に小さく学習する。

##### RMSProp

Adagrad では学習が進むにつれて学習係数が小さくなってしまう問題があり、それを回避する手法。  
Adagrad の問題だった学習が進むにつれて分母の合計値は増えていき学習係数が 0 に近づいていくことを過去何回かの合計値を近似する指数移動平均を使用することで解消する。

##### Adam

移動平均で振動を抑制するモーメンタム と 学習率を調整して振動を抑制する RMSProp を組み合わせている

##### 演習

[演習結果](../pdf/optimizer.pdf "演習結果")

#### 過学習

テスト誤差と訓練誤差とで、学習曲線が乖離すること。  
特定の訓練サンプルに対して、特化して学習してしまうこと。  
原因：パラメータの数が多い、パラメータの値が適切でない、ノードが多いなど  
⇒ ネットワークの自由度が高いともいえる

##### 過学習の原因

- パラメータを大量に持ち、表現力の高いモデルである事
- 訓練データが少ない事

##### 確認テスト
問：機械学習の線形モデルの正則化はモデルの重みを制限することで可能になる。線形モデルの正則化手法の中にリッジ回帰という手法がある、その特徴は？

回答：ハイパーパラメータを大きな値に設定すると、すべての重みが限りなく0に近づく

##### 過学習の対策

- 正則化
ネットワークの自由度(層数、ノード数、パラメータ値etc...)を制約すること

- Weight decay
重みが大きい値をとることで、過学習が発生することがある  
誤差に対して、正則化項を加算することで、重みを抑制  
過学習がおこりそうな重みの大きさ以下で、  
重みをコントロールし、かつ重みの大きさにばらつきを出す必要がある

- ドロップアウト
ランダムにノードを削除して学習させること  
データ量を変化させずに、異なるモデルを学習させていると解釈できる

##### 確認テスト

![問1](../images/q_1.png "問1")

回答：４ (param)  
L2ノルムは||param||^2である。paramで微分して勾配を求めると2*param。  
paramの係数である2は正則化の係数に吸収されても変わらないため、paramが正解。  

![問2](../images/q_2.png "問2")

回答：3 sign(param)  
L1ノルムの正則化項は|param|。絶対値の勾配と考えるとsign(param)となる  

![問3](../images/q_3.png "問3")

回答：4 image[top:bottom, left:right, :]  
top:bottomは画像の上から下を定義、left:rightは幅を定義、:は全ての色をそのまま利用することを表している。  
imageの形式が(縦幅、横幅、チャンネル)であることを前提としている。

##### 演習

[演習結果](../pdf/overfiting.pdf "演習結果")

#### 畳み込みニューラルネットワークの概念
- 画像処理の分野で注目
- 音声データのような時系列データも加工次第で可能
- 新しい概念として `畳み込み層` `プーリング層`が登場
- 畳み込み層の全体像：入力値×フィルター(全結合でいう重み)→出力値＋バイアス→活性化関数→出力値

##### 演習

[演習結果](../pdf/simple_convolution_network.pdf "演習結果")


#### 最新の CNN

- AlexNet
  - 2012年に開かれた画像認識コンペティションでダントツ1位になったモデル
  - ディープラーニングを採用
- 構造
  - 5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される。
- 過学習を防ぐ施策
  - サイズ4096の全結合層の出力にドロップアウトを使用している。
- 論文の注意点
  - 実験方法などは載っていないことが多い
  - 想像で補い、自分で試して見ることも重要

#### 再帰型ニューラルネットワークの概念

#### LSTM

#### GRU

#### 双方向 RNN

#### Seq2Seq

#### Word2vec

#### Attention Mechanism

#### Tensorflow の実装演習

#### 強化学習
